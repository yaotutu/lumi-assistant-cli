#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMåŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯LLMé›†æˆæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.config_manager import ConfigManager
from src.llm import OpenAILLM


async def test_llm():
    """æµ‹è¯•LLMåŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª LLMåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        
        # æ£€æŸ¥LLMé…ç½®
        if not config_manager.get_config("LLM.ENABLED", False):
            print("âŒ LLMæœªå¯ç”¨ï¼Œè¯·åœ¨config/config.jsonä¸­è®¾ç½®LLM.ENABLEDä¸ºtrue")
            return False
            
        provider = config_manager.get_config("LLM.PROVIDER", "openai")
        print(f"ğŸ“‹ LLMæä¾›å•†: {provider}")
        
        if provider == "openai":
            # è·å–OpenAIé…ç½®
            llm_config = {
                "api_key": config_manager.get_config("LLM.OPENAI_LLM.api_key", ""),
                "base_url": config_manager.get_config("LLM.OPENAI_LLM.base_url", "https://api.openai.com/v1"),
                "model": config_manager.get_config("LLM.OPENAI_LLM.model", "gpt-3.5-turbo"),
                "max_tokens": config_manager.get_config("LLM.OPENAI_LLM.max_tokens", 2000),
                "temperature": config_manager.get_config("LLM.OPENAI_LLM.temperature", 0.7)
            }
            
            print(f"ğŸ“‹ API Base URL: {llm_config['base_url']}")
            print(f"ğŸ“‹ æ¨¡å‹: {llm_config['model']}")
            
            if not llm_config["api_key"]:
                print("âŒ APIå¯†é’¥ä¸ºç©ºï¼Œè¯·åœ¨config/config.jsonä¸­è®¾ç½®LLM.OPENAI_LLM.api_key")
                return False
            
            print(f"ğŸ“‹ APIå¯†é’¥: {llm_config['api_key'][:8]}...")
            
            # åˆ›å»ºLLMå®¢æˆ·ç«¯
            print("\nğŸ”§ åˆ›å»ºLLMå®¢æˆ·ç«¯...")
            llm_client = OpenAILLM(llm_config)
            
            # æµ‹è¯•è¿æ¥
            print("ğŸ”— æµ‹è¯•è¿æ¥...")
            if await llm_client.test_connection():
                print("âœ… è¿æ¥æˆåŠŸï¼")
            else:
                print("âŒ è¿æ¥å¤±è´¥")
                return False
            
            # æµ‹è¯•ç®€å•å¯¹è¯
            print("\nğŸ’¬ æµ‹è¯•ç®€å•å¯¹è¯...")
            test_messages = [
                "ä½ å¥½",
                "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½",
                "è°¢è°¢"
            ]
            
            for i, message in enumerate(test_messages, 1):
                print(f"\nğŸ“¤ æµ‹è¯• {i}: {message}")
                try:
                    response = await llm_client.chat(message)
                    if response:
                        print(f"ğŸ“¥ å›å¤: {response}")
                    else:
                        print("âŒ æœªæ”¶åˆ°å›å¤")
                except Exception as e:
                    print(f"âŒ å¯¹è¯å‡ºé”™: {e}")
            
            # æµ‹è¯•å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæ”¯æŒï¼‰
            print("\nğŸ”§ æµ‹è¯•å·¥å…·è°ƒç”¨åŠŸèƒ½...")
            test_tools = [
                {
                    "type": "function",
                    "function": {
                        "name": "get_current_time",
                        "description": "è·å–å½“å‰æ—¶é—´",
                        "parameters": {
                            "type": "object",
                            "properties": {},
                            "required": []
                        }
                    }
                }
            ]
            
            try:
                tool_response = await llm_client.chat_with_tools(
                    "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
                    test_tools
                )
                print(f"ğŸ“¥ å·¥å…·è°ƒç”¨å›å¤: {tool_response}")
            except Exception as e:
                print(f"âš ï¸ å·¥å…·è°ƒç”¨æµ‹è¯•å¤±è´¥ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰: {e}")
            
            # å…³é—­å®¢æˆ·ç«¯
            await llm_client.close()
            print("\nâœ… LLMæµ‹è¯•å®Œæˆ")
            return True
            
        else:
            print(f"âŒ ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_config_help():
    """æ‰“å°é…ç½®å¸®åŠ©ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸ“– LLMé…ç½®è¯´æ˜")
    print("=" * 60)
    print("è¦ä½¿ç”¨LLMåŠŸèƒ½ï¼Œè¯·åœ¨config/config.jsonä¸­é…ç½®ï¼š")
    print()
    print("1. å¯ç”¨LLMåŠŸèƒ½ï¼š")
    print('   "LLM": {')
    print('     "ENABLED": true')
    print('   }')
    print()
    print("2. é…ç½®OpenAI APIï¼š")
    print('   "LLM": {')
    print('     "PROVIDER": "openai",')
    print('     "OPENAI_LLM": {')
    print('       "api_key": "ä½ çš„APIå¯†é’¥",')
    print('       "base_url": "https://api.openai.com/v1",')
    print('       "model": "gpt-3.5-turbo",')
    print('       "max_tokens": 2000,')
    print('       "temperature": 0.7')
    print('     }')
    print('   }')
    print()
    print("ğŸ’¡ æç¤ºï¼š")
    print("- å¯ä»¥ä½¿ç”¨OpenAIå…¼å®¹çš„APIæœåŠ¡")
    print("- base_urlå¯ä»¥æŒ‡å‘æœ¬åœ°éƒ¨ç½²çš„LLMæœåŠ¡")
    print("- ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œèƒ½å¤Ÿè®¿é—®APIæœåŠ¡")


async def main():
    """ä¸»å‡½æ•°"""
    success = await test_llm()
    
    if not success:
        print_config_help()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ LLMåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸ LLMåŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())